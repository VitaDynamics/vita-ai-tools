{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf0641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vita_toolkit import depth_rgb_to_pcd, project_pc_to_bev\n",
    "from vita_toolkit.point_cloud.viz import visualize_point_cloud, visualize_bev\n",
    "from vita_toolkit.lmdb_reader import simple_read_example, PureLMDBReader\n",
    "import numpy as np \n",
    "import pickle\n",
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8970f910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from file system \n",
    "from vita_toolkit.filesystem_reader import FilesystemReader\n",
    "file_path = \"/home/heng.li/repo/vita-agent/test_data\"\n",
    "reader = FilesystemReader(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c306e30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datas = list(reader.iterate_frames())\n",
    "test_data = test_datas[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1ee919",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1886ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# load image as PIL from numpy arrary \n",
    "def numpy_to_pil(image: np.ndarray):\n",
    "    print(image.shape)\n",
    "    numpy_image = np.ascontiguousarray(image)\n",
    "    # to pil \n",
    "    image = Image.fromarray(image)\n",
    "    return image, numpy_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a0b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_image, numpy_image = numpy_to_pil(test_data[\"left_image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0861e8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import DepthProImageProcessorFast, DepthProForDepthEstimation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "image = pil_image\n",
    "\n",
    "image_processor = DepthProImageProcessorFast.from_pretrained(\"apple/DepthPro-hf\")\n",
    "model = DepthProForDepthEstimation.from_pretrained(\"apple/DepthPro-hf\").to(device)\n",
    "\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "post_processed_output = image_processor.post_process_depth_estimation(\n",
    "    outputs, target_sizes=[(image.height, image.width)],\n",
    ")\n",
    "\n",
    "field_of_view = post_processed_output[0][\"field_of_view\"]\n",
    "focal_length = post_processed_output[0][\"focal_length\"]\n",
    "depth = post_processed_output[0][\"predicted_depth\"]\n",
    "numpy_depth = depth.detach().cpu().numpy()\n",
    "inverse_depth = 1 / numpy_depth\n",
    "# Visualize inverse depth instead of depth, clipped to [0.1m;250m] range for better visualization.\n",
    "max_invdepth_vizu = min(inverse_depth.max(), 1 / 0.1)\n",
    "min_invdepth_vizu = max(1 / 250, inverse_depth.min())\n",
    "inverse_depth_normalized = (inverse_depth - min_invdepth_vizu) / (\n",
    "    max_invdepth_vizu - min_invdepth_vizu\n",
    ")\n",
    "# Save as color-mapped \"turbo\" jpg image.\n",
    "cmap = plt.get_cmap(\"turbo\")\n",
    "color_depth = (cmap(inverse_depth_normalized)[..., :3] * 255).astype(np.uint8)\n",
    "depth = Image.fromarray(color_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c80d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bde3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c843ad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c96b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Camera calibration parameters\n",
    "intrinsic_params_rgb = {\n",
    "    \"fx\": 721.744,\n",
    "    \"fy\": 721.744, \n",
    "    \"cx\": 972.885,\n",
    "    \"cy\": 590.376\n",
    "}\n",
    "\n",
    "extrinsic_matrix = np.array(\n",
    "    [\n",
    "        [-0.0130459, 0.999647, 0.0231345, 0.0351892],\n",
    "        [0.065793, 0.0239446, -0.997545, 0.00365625],\n",
    "        [-0.997748, -0.0114917, -0.0660821, -0.259888],\n",
    "        [0, 0, 0, 1],\n",
    "    ]\n",
    ")\n",
    "\n",
    "extrinsic_params = {\"data\": extrinsic_matrix}\n",
    "\n",
    "# Debug: Check shapes and depth value ranges\n",
    "print(\"Original RGB image shape:\", test_data[\"left_image\"].shape)\n",
    "print(\"Depth image shape:\", numpy_depth.shape)\n",
    "print(\"Depth value range:\", numpy_depth.min(), \"to\", numpy_depth.max())\n",
    "\n",
    "# The RGB image needs to be properly formatted for Open3D\n",
    "# It should be (height, width, 3) not (3, height, width)\n",
    "rgb_image = test_data[\"left_image\"]\n",
    "if rgb_image.shape[0] == 3:  # If channels are first\n",
    "    rgb_image = np.transpose(rgb_image, (1, 2, 0))  # Move channels to last\n",
    "    \n",
    "print(\"Corrected RGB image shape:\", rgb_image.shape)\n",
    "\n",
    "# Fix 1: Ensure depth is in millimeters and within reasonable range\n",
    "# DepthPro outputs are in meters, convert to millimeters for Open3D\n",
    "depth_mm = numpy_depth * 1000\n",
    "\n",
    "# Fix 2: Clamp depth values to reasonable range (0.1m to 5m)\n",
    "depth_mm = np.clip(depth_mm, 100, 20000)  # 0.1m to 5m in mm\n",
    "\n",
    "# Fix 3: Ensure depth is uint16 format (preferred by Open3D)\n",
    "depth_mm = depth_mm.astype(np.uint16)\n",
    "\n",
    "print(\"Processed depth range:\", depth_mm.min(), \"to\", depth_mm.max(), \"mm\")\n",
    "\n",
    "# Call the function with corrected depth\n",
    "points, colors = depth_rgb_to_pcd(\n",
    "    depth=depth_mm, \n",
    "    intrinsic=intrinsic_params_rgb,\n",
    "    extrinsic=extrinsic_params,\n",
    "    rgb=numpy_image,\n",
    "    depth_rgb_scale=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31d83a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c618a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faaad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rerun as rr\n",
    "import matplotlib\n",
    "cmap = matplotlib.colormaps[\"turbo_r\"]\n",
    "norm = matplotlib.colors.Normalize(\n",
    "vmin=2.0,\n",
    "vmax=15,\n",
    ")\n",
    "# Now we viz use rerun. \n",
    "rr.init(\"vita_toolkit\")\n",
    "rr.connect_grpc(url=\"rerun+http://127.0.0.1:9876/proxy\")\n",
    "# For point cloud \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf91058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vita_toolkit.point_cloud.img_to_pc import pcd_to_camera_coordinate\n",
    "from vita_toolkit.point_cloud.pc_to_bev import filter_pcd\n",
    "t = np.array([-0.0613157 -0.0842516 -0.112423])\n",
    "# pc_range = np.array([-6.4, -6.4, -2.0, 6.4, 6.4, 2.8])\n",
    "#lcd = filter_pcd(test_data[\"point_clouds\"][:, :3], pc_range)\n",
    "before_lidar = test_data[\"point_clouds\"][:, :3]\n",
    "before_lidar[:, 1] = before_lidar[:, 1] * -1\n",
    "lidar_pcd = pcd_to_camera_coordinate(before_lidar, extrinsic_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b835ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose to corrent cooridnates \n",
    "# Current z is the y. Current y is x. Current x is z. \n",
    "# Transpose axes: z -> y, y -> x, x -> z\n",
    "# Original lidar_pcd shape: (N, 3)\n",
    "# New order: [2, 0, 1] (x, y, z) -> (z, x, y)\n",
    "lidar_pcd = lidar_pcd[:, [2, 0, 1]]\n",
    "lidar_pcd[:, 2] = lidar_pcd[:, 2] * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79bb880",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(lidar_pcd[:, 2], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38da5090",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_colors = cmap(norm(np.linalg.norm(lidar_pcd[:, :3], axis=1)))\n",
    "rr.log(\n",
    "    \"/world/ego/lidar\", \n",
    "    rr.Points3D(\n",
    "        positions=lidar_pcd[:, :3], \n",
    "        colors=point_colors\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555ac05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# align with pcd \n",
    "from vita_toolkit.point_cloud.depht_lidar_matching import align_depth_lidar\n",
    "scale_factor, valid_depth_pairs, final_lidar_depths = align_depth_lidar(numpy_depth, lidar_pcd[:, :3], intrinsic_params_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae553984",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_lidar_depths), len(valid_depth_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f66ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06829dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_colors = cmap(norm(np.linalg.norm(points[:, :3], axis=1)))\n",
    "points[:, 0] = points[:, 0] * -1\n",
    "points[:, 1] = points[:, 1] * -1\n",
    "rr.log(\n",
    "    \"/world/ego/depth_lidar\", \n",
    "    rr.Points3D(\n",
    "        positions=points[:, :3] * scale_factor,\n",
    "        colors=point_colors\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d324e13",
   "metadata": {},
   "source": [
    "## Now project to BEV view "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d552cd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_ranges = np.array([-6.4, -6.4, -2.0, 6.4, 6.4, 2.8])\n",
    "voxel_size = np.array([0.2, 0.2, 8.0])\n",
    "bev_size = project_pc_to_bev(points, pc_ranges, voxel_size, pc_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdf7a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first batch and sum across z dimension to get 2D occupancy\n",
    "occupancy_2d = np.sum(bev_size[0], axis=0)  # Shape: (ny, nx)\n",
    "\n",
    "# Find non-zero (occupied) cells\n",
    "occupied_indices = np.where(occupancy_2d > 0)\n",
    "\n",
    "# Randomly sample from occupied cells\n",
    "random_idx = np.random.randint(len(occupied_indices[0]))\n",
    "y_idx, x_idx = occupied_indices[0][random_idx], occupied_indices[1][random_idx]\n",
    "\n",
    "# Convert grid indices to world coordinates\n",
    "x_world = pc_ranges[0] + (x_idx + 0.5) * voxel_size[0]\n",
    "y_world = pc_ranges[1] + (y_idx + 0.5) * voxel_size[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9729f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fd20ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0dfac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the new depth-lidar alignment functions\n",
    "from vita_toolkit.point_cloud.depht_lidar_matching import align_depth_lidar, DepthLidarSolver, AlignmentResult\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca520ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark different depth-lidar alignment methods\n",
    "print(\"=\" * 60)\n",
    "print(\"DEPTH-LIDAR ALIGNMENT BENCHMARK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define methods to benchmark\n",
    "methods = [\n",
    "    {\"name\": \"Least Squares\", \"method\": \"least_squares\"},\n",
    "    {\"name\": \"Median Ratio\", \"method\": \"median_ratio\"},\n",
    "    {\"name\": \"Robust Least Squares (Huber)\", \"method\": \"robust_least_squares\", \"robust_loss\": \"huber\"},\n",
    "    {\"name\": \"Robust Least Squares (Soft L1)\", \"method\": \"robust_least_squares\", \"robust_loss\": \"soft_l1\"},\n",
    "    {\"name\": \"RANSAC\", \"method\": \"ransac\"},\n",
    "    {\"name\": \"Adaptive Scaling\", \"method\": \"adaptive_neighborhood\"}\n",
    "]\n",
    "\n",
    "# Store results for comparison\n",
    "benchmark_results = []\n",
    "\n",
    "for method_config in methods:\n",
    "    method_name = method_config[\"name\"]\n",
    "    method = method_config[\"method\"]\n",
    "    \n",
    "    print(f\"\\nüîç Testing {method_name}...\")\n",
    "    \n",
    "    # Time the alignment\n",
    "    start_time = time.time()\n",
    "    \n",
    "\n",
    "    # Create solver with reasonable parameters\n",
    "    solver = DepthLidarSolver(outlier_threshold=0.2, max_iterations=100)\n",
    "    \n",
    "    # Prepare solver arguments\n",
    "    solver_args = {\n",
    "        \"depth_map\": numpy_depth,\n",
    "        \"lidar_points\": lidar_pcd,\n",
    "        \"intrinsic_params\": intrinsic_params_rgb,\n",
    "        \"method\": method\n",
    "    }\n",
    "    \n",
    "    # Add robust loss if specified\n",
    "    if \"robust_loss\" in method_config:\n",
    "        solver_args[\"robust_loss\"] = method_config[\"robust_loss\"]\n",
    "    \n",
    "    # Run alignment\n",
    "    result = solver.solve(**solver_args)\n",
    "    \n",
    "    # Calculate execution time\n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    # Store results\n",
    "    benchmark_results.append({\n",
    "        \"Method\": method_name,\n",
    "        \"Scale Factor\": result.scale_factor,\n",
    "        \"RMSE\": result.rmse,\n",
    "        \"Mean Error\": result.mean_error,\n",
    "        \"Median Error\": result.median_error,\n",
    "        \"Std Error\": result.std_error,\n",
    "        \"Correspondences\": result.num_correspondences,\n",
    "        \"Outlier Ratio\": result.outlier_ratio,\n",
    "        \"Converged\": result.converged,\n",
    "        \"Iterations\": result.iterations,\n",
    "        \"Time (s)\": execution_time\n",
    "    })\n",
    "    \n",
    "    print(f\"   ‚úÖ Scale factor: {result.scale_factor:.4f}\")\n",
    "    print(f\"   üìä RMSE: {result.rmse:.4f}\")\n",
    "    print(f\"   üéØ Mean error: {result.mean_error:.4f}\")\n",
    "    print(f\"   üìà Outlier ratio: {result.outlier_ratio:.2%}\")\n",
    "    print(f\"   üî¢ Correspondences: {result.num_correspondences}\")\n",
    "    print(f\"   ‚è±Ô∏è Time: {execution_time:.4f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BENCHMARK SUMMARY\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1lmmgjtfq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for better visualization\n",
    "df_results = pd.DataFrame(benchmark_results)\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "# Find best method based on RMSE\n",
    "successful_results = df_results[df_results['RMSE'].notna()]\n",
    "if not successful_results.empty:\n",
    "    best_method = successful_results.loc[successful_results['RMSE'].idxmin()]\n",
    "    print(f\"\\nüèÜ Best method: {best_method['Method']} (RMSE: {best_method['RMSE']:.4f})\")\n",
    "    \n",
    "    # Use best method for further analysis\n",
    "    best_solver = DepthLidarSolver(outlier_threshold=0.2, max_iterations=100)\n",
    "    best_result = best_solver.solve(\n",
    "        depth_map=numpy_depth,\n",
    "        lidar_points=lidar_pcd,\n",
    "        intrinsic_params=intrinsic_params_rgb,\n",
    "        method=best_method['Method'].lower().replace(' ', '_').replace('(', '').replace(')', '').replace('huber', 'robust_least_squares')\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìà Detailed metrics for {best_method['Method']}:\")\n",
    "    metrics = best_solver.evaluate_alignment(best_result)\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"   {key}: {value:.4f}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No successful alignments found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f64ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vita-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
